<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yuchang Jiang</title>

    <meta name="author" content="Yuchang Jiang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Yuchang Jiang
                </p>
                <p class="bio">I'm a PhD student in the <a href="https://dm3l.uzh.ch/wegner/team" class="bio">EcoVision Lab, Department of Mathematical Modeling and Machine Learning, University of Zurich</a>, supervised by <a class="bio" href="https://scholar.google.de/citations?user=sxLG1rgAAAAJ&hl=en">Jan Dirk Wegner</a> and <a class="bio" href="https://scholar.google.de/citations?user=FZuNgqIAAAAJ&hl=en">Konrad Schindler</a>. 
                My research focuses on computer vision and remote sensing, with an emphasis on vegetation parameter estimation, forest type mapping, and imbalanced data challenges. I am driven by a passion for applying deep learning techniques to tackle scientific challenges.
                I have interned in <a class="bio" href="https://deepmind.google/">Google DeepMind</a>, mainly working with <a class="bio" href="https://scholar.google.com/citations?user=XaX1OGIAAAAJ&hl=en">Maxim Neumann</a> and <a class="bio" href="http://dmorris.net/">Dan Morris</a>.
                </p>
                <p class="bio">
                I earned my master's degree from ETH Zurich and my bachelor's degree from The Hong Kong Polytechnic University. During my master's studies, I specialized in computer vision and deep learning, focusing on multi-object tracking and remote sensing challenges.
                </p>
               
                <p style="text-align:center">
                  <a href="mailto:yuchang.jiang@uzh.ch">Email</a> &nbsp;/&nbsp;
                  <a href="https://github.com/SherryJYC">Github</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/yuchang-jiang/">Linkedin</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com.hk/citations?user=u9k4HecAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://bsky.app/profile/yuchangjyc.bsky.social">Bluesky</a> 
                </p>
              </td>
              <td style="padding:2.5%;width:28%;max-width:28%">
                <a href="images/yc.JPG"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/yc.JPG" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="17">
            <tr> 
              <td width="15%" valign="middle">
                <a href="https://deepmind.google/"><img style="width:100%;max-width:100%;object-fit: cover;" src="images/GDM_logo.svg.png"></a>
              </td>
              <td width="12%" valign="middle">
                <a href="https://ethz.ch/en.html"><img style="width:100%;max-width:100%;object-fit: cover;" src="images/eth_logo.svg.png"></a>
              </td>
              <td width="12%" valign="middle">
                <a href="https://www.uzh.ch/en.html"><img style="width:100%;max-width:100%;object-fit: cover;" src="images/uzh_logo.png"></a>
              </td>
              <td width="6%" valign="middle">
                <a href="https://www.polyu.edu.hk/en/"><img style="width:100%;max-width:100%;object-fit: cover;" src="images/polyu_logo.png"></a>
              </td> 
            </tr>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>News</h2>
                  <ul>
        				    <li>6 May 2025: I'll give a presentation about <a class="bio" href="https://github.com/google-deepmind/forest_typology/tree/main/forty_v1">&#127795; FORTY benchmark</a> at <a href="https://nikal.eventsair.com/nasa-esa-international-workshop-on-geospatial-ai-foundation-model-for-earth-observation-and-earth-sciences/absrtact-submission">ESA-NASA International Workshop on AI Foundation Model for EO</a> in Frascati, Italy.</li>
        				    <li>Mar 2025: one paper accepted at <a href="https://2025.ieeeigarss.org/papers.php">IEEE International Geoscience and Remote Sensing Symposium, 2025</a> </li>
        				    <li>Feb 2025: &#127759;<a class="bio" href="https://github.com/google-deepmind/jeo">JEO</a> (a ML package for remote sensing and earth observation in JAX) is online! More codes will be open soon. </li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Data Download</h2>
                </td>
              </tr>
              <tr> 
              <td width="15%" valign="middle">
                <button class="button-51" role="button">
                	<a style="color:black;font-size:16px;font-weight:700;font-family:'Avenir Next LT W01 Bold',sans-serif;" href="https://github.com/google-deepmind/forest_typology/tree/main/forty_v1">&#127795; FORTY benchmark</a>
                </button> &nbsp; &nbsp; &nbsp; &nbsp;
                <button class="button-51" role="button">
                  <a style="color:black;font-size:16px;font-weight:700;font-family:'Avenir Next LT W01 Bold',sans-serif;" href="https://zenodo.org/records/8283348">&#127956; Swiss Canopy Height Maps</a>
                </button> &nbsp; &nbsp; &nbsp; &nbsp;
              </td>
            </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in computer vision, deep learning, and their applications in remote sensing.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/forty_teaser.png' width=100%>
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://github.com/google-deepmind/forest_typology/tree/main/forty_v1">
          <span class="papertitle">Not Every Tree is A Forest: Benchmarking Forest Types from Satellite Remote Sensing</span>
        </a>
        <br>
        <strong>Yuchang Jiang</strong>, 
        Maxim Neumann
        <br>
        <em>IEEE International Geoscience and Remote Sensing Symposium</em>, 2025
        <br>
        <!-- <a href="https://arxiv.org/abs/2504.01722">arXiv</a> / -->
        <a href="https://github.com/google-deepmind/forest_typology/tree/main/forty_v1">code</a>
        <p></p>
        <p>
				ForTy is a new global-scale, multi-modal, and multi-temporal benchmark dataset designed for advancing global FORest TYpes mapping. It comprises 200,000 time series of image patches, each including Sentinel-2, Sentinel-1, climate, and elevation data. The dataset features per-pixel annotations that distinguish between three key forest types (natural forest, planted forest, tree crops).
        </p>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/gsr_teaser.png' width=100%>
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2504.01722">
          <span class="papertitle">GSR4B: Biomass Map Super-Resolution with Sentinel-1/2 Guidance</span>
        </a>
        <br>
        Kaan Karaman,
        <strong>Yuchang Jiang</strong>, 
        Damien Robert,
        Vivien Sainte Fare Garnot,
        Maria Jo√£o Santos,
        Jan Dirk Wegner
        <br>
        <em>ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences</em>, 2025
        <br>
        <a href="https://arxiv.org/abs/2504.01722">arXiv</a> /
        <a href="https://github.com/kaankaramanofficial/GSR4B">code</a>
        <p></p>
        <p>
				We propose a new way to address high-resolution above-ground biomass estimation, by leveraging both high-resolution (HR) satellite observations and existing low-resolution (LR) biomass products. We cast this problem as Guided Super-Resolution, aiming at upsampling LR biomass maps (sources) from 100 to 10 m resolution, using auxiliary HR co-registered satellite images (guides).
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/uvote.png' width=100%>
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2305.15178">
          <span class="papertitle">Uncertainty Voting Ensemble for Imbalanced Deep Regression</span>
        </a>
        <br>
        <strong>Yuchang Jiang</strong>, 
        Vivien Sainte Fare Garnot,
        Konrad Schindler, 
        Jan Dirk Wegner
        <br>
        <em>GCPR</em>, 2024
        <br>
        <a href="https://arxiv.org/abs/2305.15178">arXiv</a> /
        <a href="https://github.com/SherryJYC/UVOTE">code</a>
        <p></p>
        <p>
				UVOTE integrates recent advances in probabilistic deep learning with an ensemble approach for imbalanced regression. We replace traditional regression losses with negative log-likelihood, which also predicts sample-wise aleatoric uncertainty.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/swiss_CHM.jpg' width=100%>
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://www.sciencedirect.com/science/article/pii/S266601722300024X">
          <span class="papertitle">Accuracy and Consistency of Space-based Vegetation Height Maps for Forest Dynamics in Alpine Terrain</span>
        </a>
        <br>
        <strong>Yuchang Jiang</strong>, 
        Marius R√ºetschi, Vivien Sainte Fare Garnot, Mauro Marty, Konrad Schindler, Christian Ginzler, Jan Dirk Wegner
        <br>
        <em>Science of Remote Sensing</em>, 2023
        <br>
        <a href="https://www.sciencedirect.com/science/article/pii/S266601722300024X">paper</a> /
        <a href="https://github.com/ecovision-uzh/veg-height-map-public">code</a> /
        <a href="https://zenodo.org/records/8283348">data download</a>
        <p></p>
        <p>
        We generate annual, countrywide vegetation height maps at a 10-m ground sampling distance for the years 2017‚Äì2020 based on Sentinel-2 satellite imagery and deep learning.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/semspray.png' width=100%>
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://ec-3.org/publications/conferences/EC32022/papers/EC32022_175.pdf">
          <span class="papertitle">SemSpray: Virtual Reality As-Is Semantic Information Labeling Tool for 3D Spatial Data</span>
        </a>
        <br>
        Yiming Zhao*, Cyprien Fol*,
        <strong>Yuchang Jiang</strong>,
        Tianyu Wu, Iro Armeni
        <br>
        <em>European Conference on Computing in Construction</em>, 2022
        <br>
        <a href="https://ec-3.org/publications/conferences/EC32022/papers/EC32022_175.pdf">paper</a>/
        <a href="https://github.com/SherryJYC/VR-3D-Annotation-Tool">code</a>
        <p></p>
        <p>
        We propose Semantic Spray (Semspray), a Virtual Reality (VR) application that provides users with intuitive and handy tools to produce semantic information on as-is 3D spatial data (mesh) of buildings.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/lake_ice.gif' width=100%>
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/document/9906117/">
          <span class="papertitle">Learning a Sensor-invariant Embedding of Satellite Data: A Case Study for Lake Ice Monitoring</span>
        </a>
        <br>
        Manu Tom, 
        <strong>Yuchang Jiang</strong>, 
        Emmanuel Baltsavias, Konrad Schindler
        <br>
        <em>Transactions on Geoscience and Remote Sensing (IEEE)</em>, 2022
        <br>
        <a href="https://ieeexplore.ieee.org/document/9906117">paper</a>
        <p></p>
        <p>
        We develop a deep learning framework that learns a joint satellite embedding to fuse MODIS, VIIRS and S1-SAR satellite data for lake icce monitoring.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/dtrack.png' width=100%>
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://ethz.ch/content/dam/ethz/special-interest/baug/igp/photogrammetry-remote-sensing-dam/documents/pdf/Papers/Yuchang_Jiang_KKN.pdf">
          <span class="papertitle">Multi-Target Multi-Camera Drone Tracking</span>
        </a>
        <br>
        <strong>Yuchang Jiang</strong>
        <br>
        <em>Wissenschaftlich-‚ÄãTechnische Jahrestagung der DGPF</em>, 2022
        <br>
        <a href="https://ethz.ch/content/dam/ethz/special-interest/baug/igp/photogrammetry-remote-sensing-dam/documents/pdf/Papers/Yuchang_Jiang_KKN.pdf">paper</a>
        <p></p>
        <p>
        We propose an approach to track multiple drones in a roughly synchronized static camera network with unknown camera poses.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/cam1_right_team.gif' width=100%>
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://github.com/SherryJYC/3D-Tracking-MVS">
          <span class="papertitle">3D Player Tracking with Multi-View Stream</span>
        </a>
        <br>
        <strong>Yuchang Jiang</strong>,
        Ying Jiao, Yelan Tao, Tianyu Wu
        <br>
        <em>3DV project</em>, 2021
        <br>
        <a href="https://github.com/SherryJYC/3D-Tracking-MVS/blob/main/document/3dtracking_report_2021.pdf">paper</a> /
        <a href="https://github.com/SherryJYC/3D-Tracking-MVS">code</a>
        <p></p>
        <p>
        We construct a working pipeline for 3d scooer player position tracking, including multi-object tracking in each camera view and multi-camera association.
        </p>
      </td>
    </tr>

    <!--
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Short papers</h2>
                <p>
                  includes abstracts, tiny papers, talks...
                </p>
                  <ul>
                    <li><a class="papertitle" href="https://ethz.ch/content/dam/ethz/special-interest/baug/igp/photogrammetry-remote-sensing-dam/documents/pdf/Papers/Jiang_LivingPlanet.pdf">Annual vegetation height maps based on Sentinel-2 data ‚Äì Potential applications for the Swiss National Forest Inventory</a> <br/>
                      Marius R√ºetschi, 
                      <strong>Yuchang Jiang</strong>, Nico Lang, Alexander Becker, Lars T. Waser, Mauro Marty, Konrad Schindler, Jan Dirk Wegner, Christian Ginzler
                      <br/>
                      <em>ESA Living Planet Symposium</em> 2022
                    </li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>
     -->
          
					<table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
            <tr>
              <td>
                <h2>Misc</h2>
                <p class="bio"> I enjoy sports during feel time, mainly badmiton, squash, and swimming. My badminton team in <a href="https://www.bc-za.ch/de/">BCZA</a> is always looking for league palyers, especially female players for 2/3. Liga. Feel free to contact me if you're interested &#127992; &#127992; &#127992;
                 </p>
              </td>
            </tr>
          </tbody></table>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Thank you for the template <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
                </p>
              </td>
            </tr>
          </tbody></table>

  </body>
</html>
